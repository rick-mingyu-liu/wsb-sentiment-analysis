{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "911d312e-3625-4e74-aa46-9539983a17db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.4.1)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.7.1-cp312-none-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.7.1-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.22.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1\n",
      "    Uninstalling torch-2.4.1:\n",
      "      Successfully uninstalled torch-2.4.1\n",
      "Successfully installed sympy-1.13.3 torch-2.7.1 torchaudio-2.7.1 torchvision-0.22.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617e68b0-c908-476c-855b-36d0cb53ad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.53.2)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: evaluate in /opt/anaconda3/lib/python3.12/site-packages (0.4.5)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65f44d3-96b1-43b3-8765-20f3478250e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load SST-2 dataset (general sentiment)\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Check structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dafe7c2d-b775-4d43-999a-a2d89cd16684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0289fbae-a81f-4bcd-a4c4-b2820e99adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a0531a-1057-4b0f-9daf-d6edcaec72b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a185afc-b556-4a72-a7e3-029702617328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c49e3b7d-441e-4bce-9468-4026f1a2b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "011e51a8-5229-4490-abdf-21685b72c474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2025.6.15)\n",
      "Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f76bb0a4-502b-4d82-b817-c4cf1e9e1e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ankurzing/sentiment-analysis-for-financial-news?dataset_version_number=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 903k/903k [00:00<00:00, 7.41MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/rickliu/.cache/kagglehub/datasets/ankurzing/sentiment-analysis-for-financial-news/versions/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ankurzing/sentiment-analysis-for-financial-news\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c443a90-1be7-4461-b454-0a0a40447c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n",
      "{'label': 'neutral', 'text': 'The contract , which was signed yesterday , will run for five years and includes a two-year extension option .'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "csv_path = \"/Users/rickliu/.cache/kagglehub/datasets/ankurzing/sentiment-analysis-for-financial-news/versions/5/all-data.csv\"\n",
    "\n",
    "# Read with correct delimiter and no header\n",
    "df = pd.read_csv(csv_path, encoding=\"ISO-8859-1\", names=[\"label\", \"text\"], engine=\"python\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Train-test split\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa8a0e05-ef6d-4ba1-afb5-ab7075b7f4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4242974c51541f6b24c896869a90a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8b6cf1d2b046f0b00f92940c07233d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/970 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'text': 'The contract , which was signed yesterday , will run for five years and includes a two-year extension option .'}\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "def encode_labels(example):\n",
    "    if isinstance(example[\"label\"], str):  # Only map if it's a string\n",
    "        example[\"label\"] = label2id[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(encode_labels)\n",
    "eval_dataset = eval_dataset.map(encode_labels)\n",
    "print(train_dataset[0]) √ü\n",
    "print(set(train_dataset['label'][:20]))  # Check first 20 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90ae6e70-12cf-4454-9c43-7250c1eae062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b43e5a352547b1a9248deba84f94e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbb01b588bc421888c7d3da2945eedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/970 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36608b08-fbd1-47e5-a9ee-fc10099c45fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad1f23cf-f173-45bf-91ff-b026ee577f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ng/prfj2ls9255881q552hlmp680000gn/T/ipykernel_20544/1159644960.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa226ddc-3060-4900-a24d-fd6eaa2dd18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='729' max='729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [729/729 09:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.439800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=729, training_loss=0.3570099609542449, metrics={'train_runtime': 573.9903, 'train_samples_per_second': 20.258, 'train_steps_per_second': 1.27, 'total_flos': 764884422282240.0, 'train_loss': 0.3570099609542449, 'epoch': 3.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2b7a510-7f04-493d-adc9-708c3ca54990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./deberta-financial/tokenizer_config.json',\n",
       " './deberta-financial/special_tokens_map.json',\n",
       " './deberta-financial/spm.model',\n",
       " './deberta-financial/added_tokens.json',\n",
       " './deberta-financial/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./deberta-financial\")\n",
    "tokenizer.save_pretrained(\"./deberta-financial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87fb27ee-9078-4b1b-b50c-2015fe3de903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9875481128692627}, {'label': 'LABEL_1', 'score': 0.9957257509231567}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"./deberta-financial\", tokenizer=\"./deberta-financial\")\n",
    "\n",
    "post1 = \"Fuck NVDA. Lost 30% in one day!\"\n",
    "post2 = \"My dad invested 200k in Nvidia and plans to hold for 3 years.\"\n",
    "\n",
    "print(sentiment_pipeline([post1, post2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f1f4e5ea-7b86-4d04-a6f0-09909ea9f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_tickers(text):\n",
    "    return re.findall(r'\\b[A-Z]{2,5}\\b', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "9a4f3d03-c836-4113-94a0-28b12a8476ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NVDA', 'TSLA']"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tickers(\"Fuck NVDA. Lost 30%! Maybe TSLA is next?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "00329b28-e653-4a13-a9e2-271b7b789a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/rmisra/news-headlines-dataset-for-sarcasm-detection?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.30M/3.30M [00:00<00:00, 16.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to sarcasm dataset: /Users/rickliu/.cache/kagglehub/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection/versions/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " import kagglehub\n",
    "\n",
    "# Download sarcasm dataset\n",
    "sarcasm_path = kagglehub.dataset_download(\"rmisra/news-headlines-dataset-for-sarcasm-detection\")\n",
    "print(\"Path to sarcasm dataset:\", sarcasm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9c2c392-cd88-47a0-a0b9-177586c36e78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'zayn malik breaks his twitter silence to thank fans', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "csv_path = sarcasm_path + \"/Sarcasm_Headlines_Dataset.json\"\n",
    "\n",
    "# Load JSON dataset\n",
    "df = pd.read_json(csv_path, lines=True)\n",
    "\n",
    "# Keep only headline & is_sarcastic\n",
    "df = df[[\"headline\", \"is_sarcastic\"]].rename(columns={\"headline\": \"text\", \"is_sarcastic\": \"label\"})\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "sarcasm_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Train-test split\n",
    "sarcasm_dataset = sarcasm_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = sarcasm_dataset[\"train\"]\n",
    "eval_dataset = sarcasm_dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38958296-e1f0-41f3-9f7e-a559a8e87348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d977e7c19fae4c9db683d00935137c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00089713b5ce411aa9e2247136ea43e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ae5732c8644b62bb1bd8363d287bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d76153486414b398af033a2af1e5924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd65742e2a848018a5b44105d897990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f081f7eb3242d6a2ab361bf63d05e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9be6748f-889f-4c66-b7a5-00623a4e32ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09dea8334634f5aa96a7f59a334c2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/ng/prfj2ls9255881q552hlmp680000gn/T/ipykernel_20544/164424720.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4008' max='4008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4008/4008 16:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4008, training_loss=0.16024079364157484, metrics={'train_runtime': 1018.5377, 'train_samples_per_second': 62.934, 'train_steps_per_second': 3.935, 'total_flos': 2122823180312064.0, 'train_loss': 0.16024079364157484, 'epoch': 3.0})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sarcasm_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4462d1f4-211a-4fff-b8de-fd062c7ffafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sarcasm_detector/tokenizer_config.json',\n",
       " './sarcasm_detector/special_tokens_map.json',\n",
       " './sarcasm_detector/vocab.txt',\n",
       " './sarcasm_detector/added_tokens.json',\n",
       " './sarcasm_detector/tokenizer.json')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./sarcasm_detector\")\n",
    "tokenizer.save_pretrained(\"./sarcasm_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "fb98b429-b412-454e-a1b6-54fb014567fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment model num_labels: 3\n",
      "Sarcasm model num_labels: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"PYTORCH_MPS_DISABLE\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Paths to your locally saved models\n",
    "sentiment_model_path = \"./deberta-financial\"\n",
    "sarcasm_model_path = \"./sarcasm_detector\"\n",
    "\n",
    "# Load models on CPU explicitly\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_path, torch_dtype=torch.float32, low_cpu_mem_usage=True).to(device)\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_path)\n",
    "\n",
    "sarcasm_model = AutoModelForSequenceClassification.from_pretrained(sarcasm_model_path, torch_dtype=torch.float32, low_cpu_mem_usage=True).to(device)\n",
    "sarcasm_tokenizer = AutoTokenizer.from_pretrained(sarcasm_model_path)\n",
    "\n",
    "sentiment_labels = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"neutral\", \"LABEL_2\": \"positive\"}\n",
    "sarcasm_labels = {\"LABEL_0\": \"not sarcastic\", \"LABEL_1\": \"sarcastic\"}\n",
    "\n",
    "print(\"Sentiment model num_labels:\", sentiment_model.config.num_labels)\n",
    "print(\"Sarcasm model num_labels:\", sarcasm_model.config.num_labels)\n",
    "\n",
    "\n",
    "def contains_sarcasm_clues(text):\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    sarcasm_keywords = [\n",
    "        \"yeah right\", \"sure\", \"great job\", \"nice job\", \"amazing\", \"love it\",\n",
    "        \"brilliant\", \"fantastic\", \"wonderful\", \"oh great\", \"perfect\"\n",
    "    ]\n",
    "    sarcasm_emojis = [\"üôÑ\", \"üòÇ\", \"üòâ\", \"üôÇ\", \"ü§£\"]\n",
    "    \n",
    "    # Original checks\n",
    "    if any(kw in text_lower for kw in sarcasm_keywords):\n",
    "        return True\n",
    "    if any(emoji in text for emoji in sarcasm_emojis):\n",
    "        return True\n",
    "    if \"...\" in text or \"!!!\" in text:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    if re.search(r\"i'd .* if\", text_lower):  # Extreme statements\n",
    "        return True\n",
    "    if \"just buy in. or don't\" in text_lower:\n",
    "        return True\n",
    "    if \"i am tipsy\" in text_lower or \"i'm drunk\" in text_lower:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def get_probabilities(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "    return probs\n",
    "\n",
    "def analyze_post(text):\n",
    "    # Get sentiment and sarcasm probabilities\n",
    "    sentiment_probs = get_probabilities(sentiment_model, sentiment_tokenizer, text)\n",
    "    sentiment_idx = sentiment_probs.argmax()\n",
    "    raw_sentiment = sentiment_labels[f\"LABEL_{sentiment_idx}\"]\n",
    "\n",
    "    sarcasm_probs = get_probabilities(sarcasm_model, sarcasm_tokenizer, text)\n",
    "    sarcasm_idx = sarcasm_probs.argmax()\n",
    "    raw_sarcasm = sarcasm_labels[f\"LABEL_{sarcasm_idx}\"]\n",
    "    sarcasm_score = round(float(sarcasm_probs[1]), 4)\n",
    "\n",
    "    tickers = extract_tickers(text)\n",
    "\n",
    "    # Heuristic override for sarcasm\n",
    "    if sarcasm_score < 0.1 and contains_sarcasm_clues(text):\n",
    "        raw_sarcasm = \"sarcastic\"\n",
    "        sarcasm_score = 0.95\n",
    "\n",
    "    # Count bullish and bearish cues\n",
    "    text_lower = text.lower()\n",
    "    bullish_cues = [\"to the moon\", \"buy in\", \"long\", \"calls\", \"bullish\", \"diamond hands\", \"üöÄ\"]\n",
    "    bearish_cues = [\"at the top\", \"crash\", \"bagholder\", \"lose\", \"drop\", \"tank\", \"losing\"]\n",
    "\n",
    "    bullish_count = sum(1 for cue in bullish_cues if cue in text_lower)\n",
    "    bearish_count = sum(1 for cue in bearish_cues if cue in text_lower)\n",
    "\n",
    "\n",
    "    if bullish_count > 0 and sarcasm_score < 0.99:\n",
    "        raw_sarcasm = \"not sarcastic\"\n",
    "\n",
    "    if sarcasm_idx == 1 or contains_sarcasm_clues(text):\n",
    "        raw_sarcasm = \"sarcastic\"\n",
    "    \n",
    "    if raw_sarcasm == \"sarcastic\":\n",
    "        if raw_sentiment == \"positive\":\n",
    "            final_interpretation = \"positive (sarcastic bullish)\"\n",
    "        elif raw_sentiment == \"negative\":\n",
    "            final_interpretation = \"negative (sarcastic bearish)\"\n",
    "        else:  # neutral sarcasm ‚Üí use cue dominance\n",
    "            if bearish_count > bullish_count:\n",
    "                final_interpretation = \"negative (sarcastic bearish)\"\n",
    "            elif bullish_count > 0:\n",
    "                final_interpretation = \"positive (sarcastic bullish)\"\n",
    "            else:\n",
    "                final_interpretation = \"neutral (sarcastic unclear)\"\n",
    "    else:\n",
    "        # Non-sarcastic logic\n",
    "        if raw_sentiment == \"positive\":\n",
    "            final_interpretation = \"positive\"\n",
    "        elif raw_sentiment == \"negative\":\n",
    "            final_interpretation = \"negative\"\n",
    "        else:  # neutral\n",
    "            if bullish_count > 0:\n",
    "                final_interpretation = \"positive\"\n",
    "            elif bearish_count > 0:\n",
    "                final_interpretation = \"negative\"\n",
    "            else:\n",
    "                final_interpretation = \"neutral\"\n",
    "\n",
    "    return {\n",
    "        \"text\": text[:80] + \"...\" if len(text) > 80 else text,\n",
    "        \"tickers\": tickers, \n",
    "        \"raw_sentiment\": raw_sentiment,\n",
    "        \"raw_sarcasm\": raw_sarcasm,\n",
    "        \"sentiment_score\": round(float(sentiment_probs[sentiment_idx]), 4),\n",
    "        \"sarcasm_score\": sarcasm_score,\n",
    "        \"final_interpretation\": final_interpretation\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "aaec9192-d70b-4317-8de3-23510f4fed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Sentiment + Sarcasm Analysis:\n",
      "                                             text tickers raw_sentiment  \\\n",
      "0  Yeah, because buying at the top always works üôÑ      []       neutral   \n",
      "1                     TSLA is going to the moon üöÄ  [TSLA]       neutral   \n",
      "2          Nice job losing all our money, genius!      []      negative   \n",
      "3                                     I love TSLA  [TSLA]      positive   \n",
      "\n",
      "     raw_sarcasm  sentiment_score  sarcasm_score          final_interpretation  \n",
      "0      sarcastic           0.9832         0.9500  negative (sarcastic bearish)  \n",
      "1  not sarcastic           0.9897         0.0009                      positive  \n",
      "2      sarcastic           0.9200         0.9500  negative (sarcastic bearish)  \n",
      "3  not sarcastic           0.7002         0.0006                      positive  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚úÖ Test multiple WSB-style posts\n",
    "test_posts = [\n",
    "    \"Yeah, because buying at the top always works üôÑ\",\n",
    "    \"TSLA is going to the moon üöÄ\",\n",
    "    \"Nice job losing all our money, genius!\",\n",
    "    \"I love TSLA\"\n",
    "]\n",
    "\n",
    "# Run analysis\n",
    "results = [analyze_post(post) for post in test_posts]\n",
    " \n",
    "# Convert to DataFrame for clean view\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nüîç Sentiment + Sarcasm Analysis:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "385abd8c-f594-4e7a-9f94-bd888386bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Final Output:\n",
      "text: OPENDOOR - this isn't just a pump. \n",
      "People have been in since IPOB. Well, have y...\n",
      "tickers: ['IPOB', 'IPOB', 'IPOB', 'THEN', 'IPOB', 'IDC']\n",
      "raw_sentiment: neutral\n",
      "raw_sarcasm: sarcastic\n",
      "sentiment_score: 0.8361\n",
      "sarcasm_score: 0.95\n",
      "final_interpretation: positive (sarcastic bullish)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Display the image\n",
    "Image(filename='post1.png')\n",
    "\n",
    "\n",
    "post1 = '''OPENDOOR - this isn't just a pump. \n",
    "People have been in since IPOB. Well, have you been in IPOB so early that you were trading IPOB for profit on the low volume? \n",
    "Well, I was, since the early, early days. THEN it was announced that IPOB = Opendoor, \n",
    "which I took as a divine sign, as I'm deeply involved in Real Estate and saw the problem they solved. \n",
    "Five fucking years I've been in this thing and looked at it inside out, while averaging down.\n",
    "\n",
    "Fast forward to today. The problem has not been solved but they are making strides to get there. \n",
    "They realized that can't just go in there guns blazing. They need some Vaseline and a John Mayer, \n",
    "Sade, Keith Sweat and Juvenile Slow Motion playlist.\n",
    "\n",
    "Buckle up. I'd eat my dick if OPENDOOR doesn't compound this run with positive news. \n",
    "Just buy in. Or don't. IDC, as I am tipsy right now. Deservedly so.\n",
    "'''\n",
    "\n",
    "# Run improved analysis\n",
    "result = analyze_post(post1)\n",
    "\n",
    "print(\"\\nüîç Final Output:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "68ead01b-26a2-43d6-9aad-e683ebb5803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Final Output:\n",
      "text: \n",
      "Who else is investing in the MAMACITA portfolio?\n",
      "\n",
      "The MAMACITA portfolio is wor...\n",
      "tickers: []\n",
      "raw_sentiment: neutral\n",
      "raw_sarcasm: not sarcastic\n",
      "sentiment_score: 0.8752\n",
      "sarcasm_score: 0.0007\n",
      "final_interpretation: neutral\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "post2 = '''\n",
    "Who else is investing in the MAMACITA portfolio?\n",
    "\n",
    "The MAMACITA portfolio is working really well for me. It consists of:\n",
    "\n",
    "Microsoft\n",
    "\n",
    "Alphabet\n",
    "\n",
    "Meta\n",
    "\n",
    "Amazon\n",
    "\n",
    "Costco\n",
    "\n",
    "Invidia (Nvidia is named after the Latin 'invidia', which means 'envy'. This is what makes this portfolio a latina üíÉ)\n",
    "\n",
    "Tesla\n",
    "\n",
    "Apple\n",
    "\n",
    "I feel like these holdings say it all. If you're not investing in a latina portfolio you're missing out\n",
    "'''\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "raw_result = analyze_post(post2)\n",
    "\n",
    "print(\"\\nüîç Final Output:\")\n",
    "for key, value in raw_result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "0774cda9-4b6f-49bd-95f2-86d44cfadbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Final Output:\n",
      "text: \n",
      "$AMC 20k$ call bet for Memorial Weekend pump\n",
      "\n",
      "Update from my previous post wher...\n",
      "tickers: ['AMC', 'DUOL', 'AMC', 'DUOL', 'AMC', 'AMC']\n",
      "raw_sentiment: neutral\n",
      "raw_sarcasm: sarcastic\n",
      "sentiment_score: 0.9931\n",
      "sarcasm_score: 0.9992\n",
      "final_interpretation: positive (sarcastic bullish)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "post3 = '''\n",
    "$AMC 20k$ call bet for Memorial Weekend pump\n",
    "\n",
    "Update from my previous post where I had $DUOL puts and $AMC 2026 calls.\n",
    "\n",
    "In summary:\n",
    "\n",
    "Sold DUOL puts last week after going 542$ to 515$.\n",
    "\n",
    "Sold AMC calls for 2026 at profit\n",
    "\n",
    "Combined all profits and went all in on September calls for AMC 3.5 strike.\n",
    "\n",
    "How regarded am I? ü§î\n",
    "'''\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "raw_result = analyze_post(post3)\n",
    "\n",
    "print(\"\\nüîç Final Output:\")\n",
    "for key, value in raw_result.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "be33acba-8bec-400c-9399-2e6c82fc945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Final Output:\n",
      "text: \n",
      "$5.5k gain on 0DTE SPY calls, almost 200%\n",
      "Back to the daily grind after swingin...\n",
      "tickers: ['SPY']\n",
      "raw_sentiment: positive\n",
      "raw_sarcasm: sarcastic\n",
      "sentiment_score: 0.6513\n",
      "sarcasm_score: 0.95\n",
      "final_interpretation: positive (sarcastic bullish)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "post4 = '''\n",
    "$5.5k gain on 0DTE SPY calls, almost 200%\n",
    "Back to the daily grind after swinging for the fences and coming up short. \n",
    "One of these times I'm gonna hit it lol. Got out of this one right at the peak of the day so far.\n",
    "Now if I can only hit a play like this when I have like $40k on it....\n",
    "'''\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "raw_result = analyze_post(post4)\n",
    "\n",
    "print(\"\\nüîç Final Output:\")\n",
    "for key, value in raw_result.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "5e3c9b5b-96f3-41c9-89a2-6d0a70716320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Final Output:\n",
      "text: \n",
      "Just shy of a 5000 dollar loss. Got off easy.\n",
      "\n",
      "I thought my thesis was correct ...\n",
      "tickers: []\n",
      "raw_sentiment: negative\n",
      "raw_sarcasm: not sarcastic\n",
      "sentiment_score: 0.9635\n",
      "sarcasm_score: 0.0012\n",
      "final_interpretation: negative\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "post5 = '''\n",
    "Just shy of a 5000 dollar loss. Got off easy.\n",
    "\n",
    "I thought my thesis was correct after seeing red markets everywhere and a little baby downturn in the morning. Got shafted with green dildos the rest of the day. Fuck you and see you next week.\n",
    "\n",
    "P.S. Looks like the market is coming down now that RobinHood has autoended my position. How fun.\n",
    "\n",
    "P.P.S. u/spooky_mudler27 has convinced me to roll this. It's either valhalla or the Wendy's dumpster on Monday.\n",
    "'''\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "raw_result = analyze_post(post5)\n",
    "\n",
    "print(\"\\nüîç Final Output:\")\n",
    "for key, value in raw_result.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "1097ee4f-78e7-4e77-ae68-450d9f038b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Final Output:\n",
      "text: \n",
      "Never forget when I held options over weekend and forgot to close them out (old...\n",
      "tickers: ['WSB', 'SPY', 'ONE']\n",
      "raw_sentiment: negative\n",
      "raw_sarcasm: not sarcastic\n",
      "sentiment_score: 0.8874\n",
      "sarcasm_score: 0.0417\n",
      "final_interpretation: negative\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "post6 = '''\n",
    "Never forget when I held options over weekend and forgot to close them out (older event never posted)\n",
    "\n",
    "After that lost post it made me remember a story I think WSB would like to hear about pure regardium.\n",
    "\n",
    "It was a weekend trip to Vegas and as I‚Äôm on the plane wifi with 10min left on a Friday trading session \n",
    "I think why not do a SPY yolo $3k otm puts for a end of year weekend sell off‚Ä¶ end up having a crazy weekend in Vegas \n",
    "and have a flight on that Monday morning not thinking what day it even was. Did not even open the app all day \n",
    "until after hours to close out and missed on $35k+ profit‚Ä¶ the real suspicious thing was Robinhood did not send me\n",
    "ONE notification like they usually do about options expiring, up %, close out blah blah‚Ä¶ super sketchy \n",
    "when I go to log into Robinhood after landing Monday‚Äôs flight it makes me upload my license and ask all this info \n",
    "then just to find out I just missed a giant bag and ended up losing the $3k ü§°\n",
    "'''\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "raw_result = analyze_post(post6)\n",
    "\n",
    "print(\"\\nüîç Final Output:\")\n",
    "for key, value in raw_result.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
