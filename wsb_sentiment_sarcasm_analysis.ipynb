{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "911d312e-3625-4e74-aa46-9539983a17db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.4.1)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.7.1-cp312-none-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.22.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.7.1-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.22.1-cp312-cp312-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchaudio-2.7.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.1\n",
      "    Uninstalling torch-2.4.1:\n",
      "      Successfully uninstalled torch-2.4.1\n",
      "Successfully installed sympy-1.13.3 torch-2.7.1 torchaudio-2.7.1 torchvision-0.22.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617e68b0-c908-476c-855b-36d0cb53ad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.53.2)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: evaluate in /opt/anaconda3/lib/python3.12/site-packages (0.4.5)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65f44d3-96b1-43b3-8765-20f3478250e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load SST-2 dataset (general sentiment)\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Check structure\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dafe7c2d-b775-4d43-999a-a2d89cd16684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0289fbae-a81f-4bcd-a4c4-b2820e99adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a0531a-1057-4b0f-9daf-d6edcaec72b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a185afc-b556-4a72-a7e3-029702617328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(transformers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c49e3b7d-441e-4bce-9468-4026f1a2b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "011e51a8-5229-4490-abdf-21685b72c474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2025.6.15)\n",
      "Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f76bb0a4-502b-4d82-b817-c4cf1e9e1e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ankurzing/sentiment-analysis-for-financial-news?dataset_version_number=5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 903k/903k [00:00<00:00, 7.41MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /Users/rickliu/.cache/kagglehub/datasets/ankurzing/sentiment-analysis-for-financial-news/versions/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ankurzing/sentiment-analysis-for-financial-news\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c443a90-1be7-4461-b454-0a0a40447c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                               text\n",
      "0   neutral  According to Gran , the company has no plans t...\n",
      "1   neutral  Technopolis plans to develop in stages an area...\n",
      "2  negative  The international electronic industry company ...\n",
      "3  positive  With the new production plant the company woul...\n",
      "4  positive  According to the company 's updated strategy f...\n",
      "{'label': 'neutral', 'text': 'The contract , which was signed yesterday , will run for five years and includes a two-year extension option .'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "csv_path = \"/Users/rickliu/.cache/kagglehub/datasets/ankurzing/sentiment-analysis-for-financial-news/versions/5/all-data.csv\"\n",
    "\n",
    "# Read with correct delimiter and no header\n",
    "df = pd.read_csv(csv_path, encoding=\"ISO-8859-1\", names=[\"label\", \"text\"], engine=\"python\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Train-test split\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa8a0e05-ef6d-4ba1-afb5-ab7075b7f4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4242974c51541f6b24c896869a90a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8b6cf1d2b046f0b00f92940c07233d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/970 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'text': 'The contract , which was signed yesterday , will run for five years and includes a two-year extension option .'}\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "def encode_labels(example):\n",
    "    if isinstance(example[\"label\"], str):  # Only map if it's a string\n",
    "        example[\"label\"] = label2id[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(encode_labels)\n",
    "eval_dataset = eval_dataset.map(encode_labels)\n",
    "print(train_dataset[0]) ÃŸ\n",
    "print(set(train_dataset['label'][:20]))  # Check first 20 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90ae6e70-12cf-4454-9c43-7250c1eae062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b43e5a352547b1a9248deba84f94e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fbb01b588bc421888c7d3da2945eedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/970 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36608b08-fbd1-47e5-a9ee-fc10099c45fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad1f23cf-f173-45bf-91ff-b026ee577f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ng/prfj2ls9255881q552hlmp680000gn/T/ipykernel_20544/1159644960.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa226ddc-3060-4900-a24d-fd6eaa2dd18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='729' max='729' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [729/729 09:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.439800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=729, training_loss=0.3570099609542449, metrics={'train_runtime': 573.9903, 'train_samples_per_second': 20.258, 'train_steps_per_second': 1.27, 'total_flos': 764884422282240.0, 'train_loss': 0.3570099609542449, 'epoch': 3.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2b7a510-7f04-493d-adc9-708c3ca54990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./deberta-financial/tokenizer_config.json',\n",
       " './deberta-financial/special_tokens_map.json',\n",
       " './deberta-financial/spm.model',\n",
       " './deberta-financial/added_tokens.json',\n",
       " './deberta-financial/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./deberta-financial\")\n",
    "tokenizer.save_pretrained(\"./deberta-financial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87fb27ee-9078-4b1b-b50c-2015fe3de903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9875481128692627}, {'label': 'LABEL_1', 'score': 0.9957257509231567}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"./deberta-financial\", tokenizer=\"./deberta-financial\")\n",
    "\n",
    "post1 = \"Fuck NVDA. Lost 30% in one day!\"\n",
    "post2 = \"My dad invested 200k in Nvidia and plans to hold for 3 years.\"\n",
    "\n",
    "print(sentiment_pipeline([post1, post2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f1f4e5ea-7b86-4d04-a6f0-09909ea9f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_tickers(text):\n",
    "    return re.findall(r'\\b[A-Z]{2,5}\\b', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9a4f3d03-c836-4113-94a0-28b12a8476ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NVDA', 'TSLA']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_tickers(\"Fuck NVDA. Lost 30%! Maybe TSLA is next?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "00329b28-e653-4a13-a9e2-271b7b789a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/rmisra/news-headlines-dataset-for-sarcasm-detection?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.30M/3.30M [00:00<00:00, 16.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to sarcasm dataset: /Users/rickliu/.cache/kagglehub/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection/versions/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " import kagglehub\n",
    "\n",
    "# Download sarcasm dataset\n",
    "sarcasm_path = kagglehub.dataset_download(\"rmisra/news-headlines-dataset-for-sarcasm-detection\")\n",
    "print(\"Path to sarcasm dataset:\", sarcasm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9c2c392-cd88-47a0-a0b9-177586c36e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'zayn malik breaks his twitter silence to thank fans', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "csv_path = sarcasm_path + \"/Sarcasm_Headlines_Dataset.json\"\n",
    "\n",
    "# Load JSON dataset\n",
    "df = pd.read_json(csv_path, lines=True)\n",
    "\n",
    "# Keep only headline & is_sarcastic\n",
    "df = df[[\"headline\", \"is_sarcastic\"]].rename(columns={\"headline\": \"text\", \"is_sarcastic\": \"label\"})\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "sarcasm_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Train-test split\n",
    "sarcasm_dataset = sarcasm_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = sarcasm_dataset[\"train\"]\n",
    "eval_dataset = sarcasm_dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38958296-e1f0-41f3-9f7e-a559a8e87348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d977e7c19fae4c9db683d00935137c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00089713b5ce411aa9e2247136ea43e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ae5732c8644b62bb1bd8363d287bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d76153486414b398af033a2af1e5924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd65742e2a848018a5b44105d897990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21367 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f081f7eb3242d6a2ab361bf63d05e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9be6748f-889f-4c66-b7a5-00623a4e32ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09dea8334634f5aa96a7f59a334c2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/ng/prfj2ls9255881q552hlmp680000gn/T/ipykernel_20544/164424720.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4008' max='4008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4008/4008 16:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4008, training_loss=0.16024079364157484, metrics={'train_runtime': 1018.5377, 'train_samples_per_second': 62.934, 'train_steps_per_second': 3.935, 'total_flos': 2122823180312064.0, 'train_loss': 0.16024079364157484, 'epoch': 3.0})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sarcasm_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4462d1f4-211a-4fff-b8de-fd062c7ffafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sarcasm_detector/tokenizer_config.json',\n",
       " './sarcasm_detector/special_tokens_map.json',\n",
       " './sarcasm_detector/vocab.txt',\n",
       " './sarcasm_detector/added_tokens.json',\n",
       " './sarcasm_detector/tokenizer.json')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./sarcasm_detector\")\n",
    "tokenizer.save_pretrained(\"./sarcasm_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6961d603-758b-453c-b731-117107578124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./deberta_sentiment/tokenizer_config.json',\n",
       " './deberta_sentiment/special_tokens_map.json',\n",
       " './deberta_sentiment/vocab.txt',\n",
       " './deberta_sentiment/added_tokens.json',\n",
       " './deberta_sentiment/tokenizer.json')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save your trained DeBERTa sentiment model\n",
    "sentiment_save_path = \"./deberta_sentiment\"\n",
    "\n",
    "model.save_pretrained(sentiment_save_path)\n",
    "tokenizer.save_pretrained(sentiment_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fb98b429-b412-454e-a1b6-54fb014567fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment model path: /Users/rickliu/Desktop/deberta_sentiment\n",
      "Sarcasm model path: /Users/rickliu/Desktop/sarcasm_detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Paths to local models\n",
    "sentiment_model_path = os.path.abspath(\"./deberta_sentiment\")\n",
    "sarcasm_model_path = os.path.abspath(\"./sarcasm_detector\")\n",
    "\n",
    "print(\"Sentiment model path:\", sentiment_model_path)\n",
    "print(\"Sarcasm model path:\", sarcasm_model_path)\n",
    "\n",
    "# Load models and tokenizers\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_path, local_files_only=True)\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_path, local_files_only=True)\n",
    "\n",
    "sarcasm_model = AutoModelForSequenceClassification.from_pretrained(sarcasm_model_path, local_files_only=True)\n",
    "sarcasm_tokenizer = AutoTokenizer.from_pretrained(sarcasm_model_path, local_files_only=True)\n",
    "\n",
    "# Create HuggingFace pipelines\n",
    "sentiment_analyzer = pipeline(\"text-classification\", model=sentiment_model, tokenizer=sentiment_tokenizer)\n",
    "sarcasm_detector = pipeline(\"text-classification\", model=sarcasm_model, tokenizer=sarcasm_tokenizer)\n",
    "\n",
    "# Analyze function: Get raw model predictions\n",
    "def analyze_post(text):\n",
    "    sentiment = sentiment_analyzer(text)[0]\n",
    "    sarcasm = sarcasm_detector(text)[0]\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"sentiment\": sentiment[\"label\"],        # e.g., LABEL_0 / LABEL_1 / LABEL_2\n",
    "        \"sentiment_score\": round(sentiment[\"score\"], 4),\n",
    "        \"sarcasm\": sarcasm[\"label\"],            # e.g., LABEL_0 / LABEL_1\n",
    "        \"sarcasm_score\": round(sarcasm[\"score\"], 4)\n",
    "    }\n",
    "\n",
    "# Interpret sentiment with sarcasm logic\n",
    "def interpret_sentiment_with_sarcasm(result):\n",
    "    # Mapping model labels to human-readable labels\n",
    "    sentiment_labels = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"neutral\", \"LABEL_2\": \"positive\"}\n",
    "    sarcasm_labels = {\"LABEL_0\": \"not sarcastic\", \"LABEL_1\": \"sarcastic\"}\n",
    "    \n",
    "    # Extract values\n",
    "    raw_sentiment = sentiment_labels[result[\"sentiment\"]]\n",
    "    raw_sarcasm = sarcasm_labels[result[\"sarcasm\"]]\n",
    "    sentiment_score = result[\"sentiment_score\"]\n",
    "    sarcasm_score = result[\"sarcasm_score\"]\n",
    "    \n",
    "    # Apply sarcasm-based adjustment\n",
    "    if sarcasm_score > 0.8 and raw_sentiment == \"negative\":\n",
    "        final_interpretation = \"positive (sarcastic bullish)\"\n",
    "    else:\n",
    "        final_interpretation = raw_sentiment\n",
    "    \n",
    "    # Return structured result\n",
    "    return {\n",
    "        \"text\": result[\"text\"],\n",
    "        \"raw_sentiment\": raw_sentiment,\n",
    "        \"raw_sarcasm\": raw_sarcasm,\n",
    "        \"sentiment_score\": sentiment_score,\n",
    "        \"sarcasm_score\": sarcasm_score,\n",
    "        \"final_interpretation\": final_interpretation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "385abd8c-f594-4e7a-9f94-bd888386bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Final Output:\n",
      "text: OPENDOOR - this isn't just a pump. \n",
      "People have been in since IPOB. Well, have you been in IPOB so early that you were trading IPOB for profit on the low volume? \n",
      "Well, I was, since the early, early days. THEN it was announced that IPOB = Opendoor, \n",
      "which I took as a divine sign, as I'm deeply involved in Real Estate and saw the problem they solved. \n",
      "Five fucking years I've been in this thing and looked at it inside out, while averaging down.\n",
      "\n",
      "Fast forward to today. The problem has not been solved but they are making strides to get there. \n",
      "They realized that can't just go in there guns blazing. They need some Vaseline and a John Mayer, \n",
      "Sade, Keith Sweat and Juvenile Slow Motion playlist.\n",
      "\n",
      "Buckle up. I'd eat my dick if OPENDOOR doesn't compound this run with positive news. \n",
      "Just buy in. Or don't. IDC, as I am tipsy right now. Deservedly so.\n",
      "raw_sentiment: negative\n",
      "raw_sarcasm: not sarcastic\n",
      "sentiment_score: 0.9981\n",
      "sarcasm_score: 0.9981\n",
      "final_interpretation: positive (sarcastic bullish)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Display the image\n",
    "Image(filename='screenshot.png')\n",
    "\n",
    "\n",
    "post1 = '''OPENDOOR - this isn't just a pump. \n",
    "People have been in since IPOB. Well, have you been in IPOB so early that you were trading IPOB for profit on the low volume? \n",
    "Well, I was, since the early, early days. THEN it was announced that IPOB = Opendoor, \n",
    "which I took as a divine sign, as I'm deeply involved in Real Estate and saw the problem they solved. \n",
    "Five fucking years I've been in this thing and looked at it inside out, while averaging down.\n",
    "\n",
    "Fast forward to today. The problem has not been solved but they are making strides to get there. \n",
    "They realized that can't just go in there guns blazing. They need some Vaseline and a John Mayer, \n",
    "Sade, Keith Sweat and Juvenile Slow Motion playlist.\n",
    "\n",
    "Buckle up. I'd eat my dick if OPENDOOR doesn't compound this run with positive news. \n",
    "Just buy in. Or don't. IDC, as I am tipsy right now. Deservedly so.\n",
    "'''\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "raw_result = analyze_post(post1)\n",
    "output = interpret_sentiment_with_sarcasm(raw_result)\n",
    "\n",
    "print(\"\\nğŸ” Final Output:\")\n",
    "for key, value in output.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "68ead01b-26a2-43d6-9aad-e683ebb5803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Final Output:\n",
      "text: \n",
      "Who else is investing in the MAMACITA portfolio?\n",
      "\n",
      "The MAMACITA portfolio is working really well for me. It consists of:\n",
      "\n",
      "Microsoft\n",
      "\n",
      "Alphabet\n",
      "\n",
      "Meta\n",
      "\n",
      "Amazon\n",
      "\n",
      "Costco\n",
      "\n",
      "Invidia (Nvidia is named after the Latin 'invidia', which means 'envy'. This is what makes this portfolio a latina ğŸ’ƒ)\n",
      "\n",
      "Tesla\n",
      "\n",
      "Apple\n",
      "\n",
      "I feel like these holdings say it all. If you're not investing in a latina portfolio you're missing out\n",
      "\n",
      "raw_sentiment: negative\n",
      "raw_sarcasm: not sarcastic\n",
      "sentiment_score: 0.9993\n",
      "sarcasm_score: 0.9993\n",
      "final_interpretation: positive (sarcastic bullish)\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "post2 = '''\n",
    "Who else is investing in the MAMACITA portfolio?\n",
    "\n",
    "The MAMACITA portfolio is working really well for me. It consists of:\n",
    "\n",
    "Microsoft\n",
    "\n",
    "Alphabet\n",
    "\n",
    "Meta\n",
    "\n",
    "Amazon\n",
    "\n",
    "Costco\n",
    "\n",
    "Invidia (Nvidia is named after the Latin 'invidia', which means 'envy'. This is what makes this portfolio a latina ğŸ’ƒ)\n",
    "\n",
    "Tesla\n",
    "\n",
    "Apple\n",
    "\n",
    "I feel like these holdings say it all. If you're not investing in a latina portfolio you're missing out\n",
    "'''\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "raw_result = analyze_post(post2)\n",
    "output = interpret_sentiment_with_sarcasm(raw_result)\n",
    "\n",
    "print(\"\\nğŸ” Final Output:\")\n",
    "for key, value in output.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
